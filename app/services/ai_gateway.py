import os
import time
import random
import json
from typing import List, Dict, Any
from openai import AsyncOpenAI
from anthropic import AsyncAnthropic
from app.services.pricing_tool import pricing_tool, PRICING_TOOL_SCHEMA

class ModelGateway:
    def __init__(self):
        # Initialize clients once
        self.openai_client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.anthropic_client = AsyncAnthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
        # DeepSeek often uses an OpenAI-compatible SDK with a different base URL
        self.deepseek_client = AsyncOpenAI(
            api_key=os.getenv("DEEPSEEK_API_KEY"), 
            base_url="https://api.deepseek.com/v1"
        )

        # Prompt Versioning Registry
        self.prompts = {
            "destination_recommendation": {
                "v1": "Given preferences: {prefs}, suggest 3 destinations...",
                "v2": "You are a travel agent. Analyze these preferences: {prefs} and suggest..."
            }
        }

    async def generate_with_tools(self, system_prompt: str, user_input: str):
        """
        Executes an Agentic loop: Think -> Call Tool -> Observe Result -> Answer
        """
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_input}
        ]
        
        tools = [PRICING_TOOL_SCHEMA]
        
        # Step 1: First call to LLM to see if it needs a tool
        response = await self.openai_client.chat.completions.create(
            model="gpt-4o",
            messages=messages,
            tools=tools,
            tool_choice="auto" 
        )
        
        message = response.choices[0].message

        # Step 2: Check if the LLM wants to use a tool
        if message.tool_calls:
            print(f"ðŸ¤– Agent decided to call tool: {message.tool_calls[0].function.name}")
            
            # Append the LLM's "intent" to history
            messages.append(message)

            # Step 3: Execute the tool
            for tool_call in message.tool_calls:
                if tool_call.function.name == "get_flight_price":
                    # Parse arguments generated by AI
                    args = json.loads(tool_call.function.arguments)
                    
                    # Call our Python service
                    tool_result = pricing_tool.search_flights(
                        origin=args["origin"],
                        destination=args["destination"],
                        date=args["date"]
                    )
                    
                    # Step 4: Feed the result back to the LLM
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call.id,
                        "content": tool_result
                    })

            # Step 5: Final call to LLM to digest the data and answer user
            final_response = await self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=messages
            )
            return final_response.choices[0].message.content
        
        return message.content

    async def generate(self, task_type: str, context: Dict, provider: str = "auto"):
        """
        Unified interface for generation.
        provider: 'openai', 'anthropic', 'deepseek', or 'auto' (AB testing/Cost routing)
        """
        
        # 1. Select Prompt Version (Simple AB Test Logic)
        prompt_version = "v2" if random.random() > 0.5 else "v1"
        prompt_template = self.prompts.get(task_type, {}).get(prompt_version)
        formatted_prompt = prompt_template.format(**context)
        
        start_time = time.time()
        response = None
        model_used = ""

        # 2. Intelligent Routing
        if provider == "auto":
            # Example: Route complex reasoning to Claude, simple lists to DeepSeek (cheaper)
            if task_type == "complex_itinerary":
                provider = "anthropic"
            else:
                provider = "deepseek"

        # 3. Provider Abstraction
        try:
            if provider == "openai":
                model_used = "gpt-4o"
                res = await self.openai_client.chat.completions.create(
                    model=model_used,
                    messages=[{"role": "user", "content": formatted_prompt}]
                )
                response = res.choices[0].message.content

            elif provider == "anthropic":
                model_used = "claude-3-5-sonnet-20240620"
                res = await self.anthropic_client.messages.create(
                    model=model_used,
                    max_tokens=1024,
                    messages=[{"role": "user", "content": formatted_prompt}]
                )
                response = res.content[0].text

            elif provider == "deepseek":
                model_used = "deepseek-chat"
                res = await self.deepseek_client.chat.completions.create(
                    model=model_used,
                    messages=[{"role": "user", "content": formatted_prompt}]
                )
                response = res.choices[0].message.content

        except Exception as e:
            # Fallback logic could go here
            raise e

        # 4. Log Metrics (Latency, Cost, Provider)
        duration = time.time() - start_time
        self._log_metrics(provider, model_used, duration, prompt_version)
        
        return response

    def _log_metrics(self, provider, model, duration, prompt_version):
        # In production, send this to Prometheus/Datadog
        print(f"[METRIC] Provider: {provider} | Model: {model} | Time: {duration:.2f}s | Prompt: {prompt_version}")

def get_gateway():
    return ModelGateway()